question6
===========

##描述
You have a billion urls ,where each is a huge page. How do you detect the duplicate documents?
你有10亿个url，每个url对应一个非常大的网页，你要如何检测重复的网页

##解答
对每个网页产生一个hash值，判断是否有重复的hash值就行了。这道题目理解起来很容易，但是实施起来就复杂了。url数量太大，所以，如何存储，
存储之后如何读取都是问题。以及hash值应该如何设计，才能够保证快速的进行比对。

hash值的设计。网页对应的都是html格式的文本，所以，首先要将所有的标记元素去除，只剩下纯文本，然后对纯文本进行md5或者SHA-1操作，产生
一个唯一的hash值
hash值的存储，必然是涉及多台机器的，所以，考虑使用一致性hash算法对hash值再次进行hash，将具体的某一个hash值分配到某一台机器上进行存储
使用一致性hash算法的原因是方便添加新的机器。

##扩展
对于这道题目，其实只是说到了重复网页，其实有时候还应该考虑的是相似性网页，也就是说两个网页的相似性非常高。这样就涉及到了文本相似度的问题了。
一般而言，对于文本相似度采用TF-IDF进行计算，但是TF-IDF计算文本相似度比较费时，同时数据维度会很大很大。所以，要考虑到降维，即将维度降低
而要实现降维后的相似度计算，一般采用semi-hash来进行降维，这样就可以保持高效率的对文本的相似度进行计算。